---
title: "Text as Data: Problem Set 01"
author: "Helen Wang"
execute: 
  error: true
format:
  html
editor_options: 
  chunk_output_type: console
theme: default
---

## Instructions

To complete this homework, you have two options.

You can either complete this assignment using a **dataset of your choice**. This is a great opportunity for you to start working with a dataset that you can potentially use for your final project.

Your second option is to this dataset of social media posts from political candidates running for Congress during the 2022 U.S. Midterm election.

You can download the dataset [here](https://www.dropbox.com/scl/fi/jz14130dlzj5xvca9txj4/midterm_candidates_labeled_all_May05.csv?rlkey=b0m1h6kk1af3uz9tjqyelthh3&dl=0)

To know more about the dataset, read and cite this paper: <https://journals.sagepub.com/doi/full/10.1177/20563051251337541>

If you want say thanks to the people who collected and shared this data with us for this class, please send a email to my colleague [Maggie MacDonald](https://maggiegmacdonald.com/)

**IMPORTANT:** Remember to NOT commit your data to github. Github does not allow you to push large datasets to remote repositories. If you commit your dataset, you will need to reset your commit, and that's always a bit of work. In addition, remember that your notebook should be compiled with the results of the code blocks.


Gen AI Use:

ChatGPT was consulted to deal with memory issues related to calling multiple textstat_simil(). Code was edited in response to reduce memory use, main changes being computing only one similarity matrix rather than 10 (1 for each reference document) and to see which similarity measure would be more memory-efficient. Also was consulted for Q6.2 in regards of how to run KIWC.


```{r}
setwd("C:/Users/helen/OneDrive/Documents/Georgetown/F2025/text_analysis/problem-set-01-2025-helenw566/")

pacman::p_load(
  dplyr,
  purrr,
  tidyr,
  ggwordcloud,
  quanteda,
  quanteda.textstats,
  preText,
  tidytext,
  quanteda.sentiment,
  quanteda.dictionaries
)
```

## Question 1

Take a small a sample of your documents and read them carefully. This sample doesn't need to be random. Select cases that are more interesting, and close to your theoretical interest in this dataset. What are you thoughts about these documents? What did you learn after reading them?

```{r}
# read in file
df <- read.csv("data/midterm_candidates_labeled_all_May05.csv")

# look at a random sample
df %>%
  slice_sample(n = 25) %>%
  pull(text)

```

The content of the documents seem to vary widely--while largely written with a political agenda in mind, the texts seem to include everything from descriptive statistics, personal attacks against select politicians, support for political allies, calls of action for specific agendas, dissemination of campaign ideals/logistics, personal social interactions, etc. As expected from social media data, there's liberal use of hashtags, emojis, referencing to specific accounts, and links, which will have to be considered during preprocessing.

## Question 2

Tokenize your documents and pre-process them, removing any "extraneous" content you noticed in closely reading a sample of your documents. What content have you removed and why? Any pre-processing steps from what you saw in class that you consider not to use here?

```{r}
df_corpus <- df %>%
  #removing NA text
  filter(!is.na(text)) %>%
  #convert to corpus
  corpus(text_field = "text") %>%
  #remove punctuation and numbers
  tokens(
    remove_punct = TRUE,
    remove_numbers = TRUE, 
    verbose = TRUE
  ) %>%
  #lowercase
  tokens_tolower() %>%
  #remove stopwords
  tokens_remove(stopwords("en")) %>%
  #stemming
  tokens_wordstem() %>%
  #convert to dfm
  dfm() 

df_corpus_trim <- df_corpus %>%
  #trim for common and rare words
  dfm_trim(min_docfreq = 0.0005,
             max_docfreq = 0.99,
             docfreq_type ="prop", 
             verbose = TRUE)  
```

In this case, I primarily removed punctuation, numbers, stop words, and also trimmed the document to remove common words and rare words. I also did some equivalence mapping via stemming. While removing symbols were an option, given that we're working with social media data, I thought keep hashtags and @s could allow us to retain important context so I retained those. I also avoid lemmatization as I already had conducted stemming and I did not want to reduce the complexity of the document too much by doing both. When trimming the document, I applied a much stricter bench mark for minimum document frequency versus maximum document frequency. While playing around with different options, I noticed that minimum document frequency appeared to play a more significant role in trimming (i.e. changing max_docfreq from 0.99 to 0.999 while holding min_docfreq constant had no effect while changing min_docfreq from 0.0001 to 0.0005 while holding max_docfreq constant led to a change from 9947 features to 3619). To reduce noise and to reduce computational expense, I took on a moderate approach to document trimming.

## Question 3

Using [Danny and Spirling's Pretext](https://github.com/matthewjdenny/preText), tell me which of the pre-processing steps makes a more substantive difference in the data transformation? Would you keep all these steps or would you follow the PreText recommendations? Use a sample of the data to solve this question.

```{r}
#preprocessing combinations
preTextDoc <- factorial_preprocessing(
  df %>% filter(!is.na(text)) %>% slice_sample(n = 100) %>% corpus(text_field = "text"),
  use_ngrams = F,
  infrequent_term_threshold = 0.2
)

#getting pretext scores
preTextResults <- preText(preTextDoc,
                           dataset_name = "Midterm Candidate Social Media Posts",
                           distance_method = "cosine",
                           num_comparisons = 20)

# regression
regression_coefficient_plot(preTextResults,
                            remove_intercept = TRUE)
```

In this case, it appears "Remove Infrequent Terms" is the one that will lead to the most substantive differences when it comes to preprocessing, as it is the only one that is both (1) positive and (2) significantly different. The rest appear to result in nonsignificant differences. In this case, my approach largely conforms with preText's analysis. I'm using largely the same preprocessing approach and I am also taking a moderate approach to removing infrequent terms. However, I would elect to still remove some infrequent terms (rather than avoid taking them out altogether) given the high dimensionality of the current dataset. Retaining all 141,296 tokens is impractical and too computationally expensive to really be a feasible option.

## Question 4

Pick an important source of variation in your data (for example: date, author identity, location, etc.). Subset the data along this dimension, and discuss which words discriminate better each group. You can do this by using TF-IDF, PMI, log(share $word_i$ in group a/share $word_i$ in b), or create a measure similar to Ban's article.

```{r}
#checking options for variation
df_corpus_trim@docvars %>% colnames()

#using tf-idf so see which words discriminate better for each group
tfidf_terms <- df_corpus_trim %>% 
  #performing tf-idf
  dfm_tfidf(scheme_tf = "prop") %>% 
  #group by candidate status
  dfm_group(candidate.status, force = TRUE) %>%
  #convert and pivtot
  convert(to = "data.frame") %>% 
  pivot_longer(
    cols = !doc_id,
    names_to = "word",
    values_to = "tf_idf"
  ) %>%
  rename(status = doc_id) %>% 
  group_by(status) %>%
  #check top 10 words
  arrange(desc(tf_idf)) %>%
  slice_head(n = 10)

#plotting word clouds to visualize
tfidf_terms %>%
  ggplot(aes(
    label = word, color = tf_idf)
    ) +
  geom_text_wordcloud()  +
  theme_minimal() +
  facet_wrap(~status) +
  labs(title = "Most Common Words by Election Denial Status") +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5)
  ) 

```

Here, I'm using "Candidate Status" as source of variation. We see that those who were on the ballot appear to be using language more centered on spurring voters to vote, notably having multiple references to "elect", "day", and "vote." It also seemed to cater more towards thanking voters for their support. In contrast, candidates that were not on the ballot appeared more likely to use language centered around some kind of announcement, notably "announce" and the most common words seem to suggest that their declaration was tied to removing someone from office or "trump", either as a result of his endorsement or otherwise. 

## Question 5

Create a dictionary (or use a pre-existing dictionary) to measure a topic of interest in your data. It can be anything, sentiment, tone, misinformation, any topic of interest. Label your documents, and visualize the prevalence of your classes.

```{r}
#adding sentiment scores
sentiment_doc <- docvars(df_corpus_trim) %>% 
  #adding document id
  mutate(doc_id = docnames(df_corpus_trim)) %>%
  #adding dictionary counts
  merge(
    df_corpus_trim %>% 
      dfm_lookup(dictionary = data_dictionary_LSD2015[1:2]) %>% 
      convert(to = "data.frame"),
    by = "doc_id"
  ) 

sentiment_doc %>%
  #determining top sentiment
  mutate(
    major_sentiment = ifelse(
      positive > negative, "Positive", ifelse(
        negative > positive, "Negative", "Neutral"
      )
    )
  ) %>%
  group_by(major_sentiment) %>%
  #counting documents by sentiment
  summarise(count = n()) %>%
  #plotting
  ggplot() +
  geom_col(aes(x = major_sentiment, y = count, fill = major_sentiment), alpha = 0.7) +
  labs(
    x = "Major Sentiment",
    y = "Count",
    title = "Major Sentiment across Social Media Posts by Count",
    subtitle = "2022 Midterm Elections",
    fill = "Sentiment"
  ) +
  theme_minimal()
```


## Question 6

Pick some documents (at least 10 for each class) that are exemplar (high probability) of being for each class of your dictionary. Read these documents. Now let's try to augment a little your classifier.

```{r}
set.seed(88)

#get documents with high prob
sample_sent <- sentiment_doc %>%
  #only getting documents with one sentiment and high counts for it
  filter((positive == 0 | negative == 0) & !(positive == 0 & negative == 0)) %>%
  mutate(total =  positive + negative) %>%
  filter(total > 5) %>% 
  pivot_longer(
    cols = c("positive", "negative"),
    names_to = "class"
  ) %>% 
  filter(value != 0) %>%
  group_by(class) %>%
  #randomly select 10 for each class
  slice_sample(n = 10) 

#retrieve original text 
sample_text <- df %>% 
  merge(
    sample_sent, 
    by = "X",
    all.y = T
  ) %>%
  select(class, doc_id, X, text) 

sample_text %>% pull(text)
```


### Question 6.1

Using cosine similarity, grab the closest 10 documents to each reference document you read before. Read those. How well does the cosine similarity work? Try with another measure of distance. Anything interesting there?

##### Cosine

```{r}
#computing cosine similarity in ref to docs
closest_docs <- textstat_simil(
    df_corpus_trim, 
    y = df_corpus_trim[sample_sent$doc_id,],
    margin = "documents",
    method = "cosine"
  ) %>%
  as.data.frame() %>%
  group_by(document2) %>%
  #take the top closest docs
  arrange(desc(cosine)) %>%
  slice_head(n = 10) %>%
  rename(
    doc = document1,
    ref_doc = document2,
    cosine = cosine
  ) 

#add text and doc to docvars
docvars(df_corpus_trim, "text") <- df[df$X %in% docvars(df_corpus_trim)$X, "text"]
docvars(df_corpus_trim, "doc") <- df_corpus_trim@Dimnames["docs"]

#add text
closest_docs %>%
  #adding text
  merge(
    docvars(df_corpus_trim) %>% select(doc, text),
    by = "doc"
  ) %>% 
  #adding reference text
  merge(
    sample_text %>% rename(ref_doc = doc_id, ref_text = text), 
    by = "ref_doc",
    all.x = T
  ) %>% 
  select(class, ref_doc, ref_text, doc, text) %>% View()

```

Using cosine similarity, it appears that the top 10 most similar documents generally appear to have similar sentiments and/or matching topics to the original reference document. This appears somewhat dependent on the overall "extremism" of the original reference document. Reference document that appears more obvious positive or sentiment seem more likely to have "similar" documents that have matching sentiments. Similarly, reference documents that seem clearly about a specific topic appear to be more likely to have matching content across their "similar" documents. In contrast, reference documents that might use more common words or are more neutral in sentiment, seem to vary a lot in terms of their "similar documents." In those cases, the similarities are harder to identify. Neither the content, tone, or overall sentiment seemed similar at first glance. For example, one reference document containing critiques on a specific politician was notably said to be "similar" to other documents that were notably more positive in sentiment--containing documents thanking constituents for their support. What is similar about these documents is not obviously apparent.

##### Correlation

```{r}
#computing correlation in ref to docs
closest_docs_cor <- textstat_simil(
    df_corpus_trim, 
    y = df_corpus_trim[sample_sent$doc_id,],
    margin = "documents",
    method = "correlation"
  ) %>%
  as.data.frame() %>%
  group_by(document2) %>%
  #take the top closest docs
  arrange(desc(correlation)) %>%
  slice_head(n = 10) %>%
  rename(
    doc = document1,
    ref_doc = document2,
    correlation = correlation
  ) 

#add text
closest_docs_cor %>%
  #add text
  merge(
    docvars(df_corpus_trim) %>% select(doc, text),
    by = "doc"
  ) %>% 
  #add ref text
  merge(
    sample_text %>% rename(ref_doc = doc_id, ref_text = text), 
    by = "ref_doc",
    all.x = T
  ) %>% 
  select(class, ref_doc, ref_text, doc, text) %>% View()
```

Here, I utilized correlation as an alternative measure of similarity. The results seem comparable with cosine similarity, with many of the same documents being listed as similar. Correlation introduces some new documents and omits others that cosine similarity had marked as similar but the new inclusions overall seem obvious in why they were considered similar. While some new additions are dubious, there are no more dubious than the one's marked similar via cosine similarity.

### Question 6.2

Now, check qualitative these documents. Take a look at the top features, keyword in Context, higher TF-IDF. Would you change anything in your dictionary now that you looked in these documents?

```{r}
#check top features

topfeat <- dfm_subset(df_corpus_trim, subset = docnames(df_corpus_trim) %in% closest_docs$doc) %>% topfeatures()

topfeat

#tf-idf
dfm_subset(df_corpus_trim, subset = docnames(df_corpus_trim) %in% closest_docs$doc) %>%
  dfm_tfidf() %>%
  topfeatures()

#keywords in context
kic <- as.data.frame(matrix(ncol = 7, nrow = 0))

for (word in names(topfeat)) {
  #corpus of the similar documents, tokenize to meet kwic requirements
  temp_corpus <- corpus(docvars(dfm_subset(df_corpus_trim, subset = docnames(df_corpus_trim) %in% closest_docs$doc))$text) %>% 
    tokens(remove_punct = TRUE,
           remove_numbers = TRUE)
  #compute kiwc
  context <- kwic(temp_corpus, pattern = word, window = 5)
  #add to df
  kic <- rbind(kic, context)
}

kic %>% head()

```

I would consider integrating some of the top features into the dictionary. While some words, such as "crime" are already marked as "negative sentiment" in the dictionary, words such as "famili" and its equivalents are not. Given its frequency and importance, future sentiment analyses using this dataset should consider it when labeling sentiments.

